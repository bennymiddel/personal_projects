{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and cleaning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is from this notebook: https://www.kaggle.com/code/durgeshrao9993/deep-learning-terrain-classification/notebook\n",
    "\n",
    "I adjust my environment with the needed packages and run this code to improve the image selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMidd\\.conda\\envs\\image_recognition\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m mean, std, im_size \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], [\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m], \u001b[38;5;241m224\u001b[39m\n\u001b[0;32m     50\u001b[0m tfs \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mCompose([T\u001b[38;5;241m.\u001b[39mResize((im_size, im_size)), T\u001b[38;5;241m.\u001b[39mToTensor(), T\u001b[38;5;241m.\u001b[39mNormalize(mean \u001b[38;5;241m=\u001b[39m mean, std \u001b[38;5;241m=\u001b[39m std)])\n\u001b[1;32m---> 51\u001b[0m tr_dl, val_dl, ts_dl, classes \u001b[38;5;241m=\u001b[39m \u001b[43mget_dls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tr_dl)); \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(val_dl)); \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ts_dl)); \u001b[38;5;28mprint\u001b[39m(classes)\n",
      "Cell \u001b[1;32mIn[1], line 44\u001b[0m, in \u001b[0;36mget_dls\u001b[1;34m(root, transformations, bs, split, ns)\u001b[0m\n\u001b[0;32m     40\u001b[0m ts_len \u001b[38;5;241m=\u001b[39m total_len \u001b[38;5;241m-\u001b[39m (tr_len \u001b[38;5;241m+\u001b[39m vl_len)\n\u001b[0;32m     42\u001b[0m tr_ds, vl_ds, ts_ds \u001b[38;5;241m=\u001b[39m random_split(dataset \u001b[38;5;241m=\u001b[39m ds, lengths \u001b[38;5;241m=\u001b[39m [tr_len, vl_len, ts_len])\n\u001b[1;32m---> 44\u001b[0m tr_dl, val_dl, ts_dl \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mns\u001b[49m\u001b[43m)\u001b[49m, DataLoader(vl_ds, batch_size \u001b[38;5;241m=\u001b[39m bs, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, num_workers \u001b[38;5;241m=\u001b[39m ns), DataLoader(ts_ds, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, num_workers \u001b[38;5;241m=\u001b[39m ns)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tr_dl, val_dl, ts_dl, ds\u001b[38;5;241m.\u001b[39mcls_names\n",
      "File \u001b[1;32mc:\\Users\\BMidd\\.conda\\envs\\image_recognition\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:350\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 350\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BMidd\\.conda\\envs\\image_recognition\\lib\\site-packages\\torch\\utils\\data\\sampler.py:143\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "import os, torch, shutil, numpy as np\n",
    "from glob import glob; from PIL import Image\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "torch.manual_seed(2024)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root, transformations = None, im_files = [\".jpg\", \".jpeg\", \".png\"]):\n",
    "        \n",
    "        self.transformations = transformations\n",
    "        self.im_paths = sorted(glob(f\"{root}/*/*{[im_file for im_file in im_files]}\"))\n",
    "        \n",
    "        self.cls_names, self.cls_counts, count, data_count = {}, {}, 0, 0\n",
    "        for idx, im_path in enumerate(self.im_paths):\n",
    "            class_name = self.get_class(im_path)\n",
    "            if class_name not in self.cls_names: self.cls_names[class_name] = count; self.cls_counts[class_name] = 1; count += 1\n",
    "            else: self.cls_counts[class_name] += 1\n",
    "        \n",
    "    def get_class(self, path): return os.path.dirname(path).split(\"/\")[-1]\n",
    "    \n",
    "    def __len__(self): return len(self.im_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        im_path = self.im_paths[idx]\n",
    "        im = Image.open(im_path).convert(\"RGB\")\n",
    "        gt = self.cls_names[self.get_class(im_path)]\n",
    "        \n",
    "        if self.transformations is not None: im = self.transformations(im)\n",
    "        \n",
    "        return im, gt\n",
    "    \n",
    "def get_dls(root, transformations, bs, split = [0.9, 0.05, 0.05], ns = 4):\n",
    "    \n",
    "    ds = CustomDataset(root = root, transformations = transformations)\n",
    "    total_len = len(ds)\n",
    "    tr_len = int(total_len * split[0])\n",
    "    vl_len = int(total_len * split[1])\n",
    "    ts_len = total_len - (tr_len + vl_len)\n",
    "    \n",
    "    tr_ds, vl_ds, ts_ds = random_split(dataset = ds, lengths = [tr_len, vl_len, ts_len])\n",
    "    \n",
    "    tr_dl, val_dl, ts_dl = DataLoader(tr_ds, batch_size = bs, shuffle = True, num_workers = ns), DataLoader(vl_ds, batch_size = bs, shuffle = False, num_workers = ns), DataLoader(ts_ds, batch_size = 1, shuffle = False, num_workers = ns)\n",
    "    \n",
    "    return tr_dl, val_dl, ts_dl, ds.cls_names\n",
    "\n",
    "root = \"/data/SIH_NN\"\n",
    "mean, std, im_size = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225], 224\n",
    "tfs = T.Compose([T.Resize((im_size, im_size)), T.ToTensor(), T.Normalize(mean = mean, std = std)])\n",
    "tr_dl, val_dl, ts_dl, classes = get_dls(root = root, transformations = tfs, bs = 32)\n",
    "\n",
    "print(len(tr_dl)); print(len(val_dl)); print(len(ts_dl)); print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_analysis(root, transformations):\n",
    "    \n",
    "    ds = CustomDataset(root = root, transformations = transformations)\n",
    "    cls_counts, width, text_width = ds.cls_counts,  0.7, 0.05\n",
    "    text_height = 2\n",
    "    cls_names = list(cls_counts.keys()); counts = list(cls_counts.values())\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (20, 10))\n",
    "    indices = np.arange(len(counts))\n",
    "\n",
    "    ax.bar(indices, counts, width, color = \"firebrick\")\n",
    "    ax.set_xlabel(\"Class Names\", color = \"red\")\n",
    "    ax.set_xticklabels(cls_names, rotation = 60)\n",
    "    ax.set(xticks = indices, xticklabels = cls_names)\n",
    "    ax.set_ylabel(\"Data Counts\", color = \"red\")\n",
    "    ax.set_title(f\"Dataset Class Imbalance Analysis\")\n",
    "\n",
    "    for i, v in enumerate(counts): ax.text(i - text_width, v + text_height, str(v), color = \"royalblue\")\n",
    "    \n",
    "data_analysis(root = root, transformations = tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_2_im(t, t_type = \"rgb\"):\n",
    "    \n",
    "    gray_tfs = T.Compose([T.Normalize(mean = [ 0.], std = [1/0.5]), T.Normalize(mean = [-0.5], std = [1])])\n",
    "    rgb_tfs = T.Compose([T.Normalize(mean = [ 0., 0., 0. ], std = [ 1/0.229, 1/0.224, 1/0.225 ]), T.Normalize(mean = [ -0.485, -0.456, -0.406 ], std = [ 1., 1., 1. ])])\n",
    "    \n",
    "    invTrans = gray_tfs if t_type == \"gray\" else rgb_tfs \n",
    "    \n",
    "    return (invTrans(t) * 255).detach().squeeze().cpu().permute(1,2,0).numpy().astype(np.uint8) if t_type == \"gray\" else (invTrans(t) * 255).detach().cpu().permute(1,2,0).numpy().astype(np.uint8)\n",
    "\n",
    "def visualize(data, n_ims, rows, cmap = None, cls_names = None):\n",
    "    \n",
    "    assert cmap in [\"rgb\", \"gray\"], \"Rasmni oq-qora yoki rangli ekanini aniqlashtirib bering!\"\n",
    "    if cmap == \"rgb\": cmap = \"viridis\"\n",
    "    \n",
    "    plt.figure(figsize = (20, 10))\n",
    "    indekslar = [random.randint(0, len(data) - 1) for _ in range(n_ims)]\n",
    "    for idx, indeks in enumerate(indekslar):\n",
    "        \n",
    "        im, gt = data[indeks]\n",
    "        # Start plot\n",
    "        plt.subplot(rows, n_ims // rows, idx + 1)\n",
    "        if cmap: plt.imshow(tensor_2_im(im, cmap), cmap=cmap)\n",
    "        else: plt.imshow(tensor_2_im(im))\n",
    "        plt.axis('off')\n",
    "        if cls_names is not None: plt.title(f\"GT -> {cls_names[int(gt)]}\")\n",
    "        else: plt.title(f\"GT -> {gt}\")\n",
    "            \n",
    "visualize(tr_dl.dataset, 20, 4, \"rgb\", list(classes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(ts_dl.dataset, 20, 4, \"rgb\", list(classes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm, torchmetrics\n",
    "from tqdm import tqdm\n",
    "m = timm.create_model(\"rexnet_150\", pretrained = True, num_classes = len(classes))  \n",
    "\n",
    "def train_setup(m): return m.to(\"cuda\").eval(), 10, \"cuda\", torch.nn.CrossEntropyLoss(), torch.optim.Adam(params = m.parameters(), lr = 3e-4)\n",
    "def to_device(batch, device): return batch[0].to(device), batch[1].to(device)\n",
    "def get_metrics(model, ims, gts, loss_fn, epoch_loss, epoch_acc, epoch_f1): preds = model(ims); loss = loss_fn(preds, gts); return loss, epoch_loss + (loss.item()), epoch_acc + (torch.argmax(preds, dim = 1) == gts).sum().item(), epoch_f1 + f1_score(preds, gts)\n",
    "\n",
    "m, epochs, device, loss_fn, optimizer = train_setup(m)\n",
    "\n",
    "f1_score = torchmetrics.F1Score(task = \"multiclass\", num_classes = len(classes)).to(device)\n",
    "save_prefix, save_dir = \"terrain\", \"saved_models\"\n",
    "print(\"Start training...\")\n",
    "best_acc, best_loss, threshold, not_improved, patience = 0, float(\"inf\"), 0.01, 0, 5\n",
    "tr_losses, val_losses, tr_accs, val_accs, tr_f1s, val_f1s = [], [], [], [], [], []\n",
    "\n",
    "best_loss = float(torch.inf)\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "\n",
    "    epoch_loss, epoch_acc, epoch_f1 = 0, 0, 0\n",
    "    for idx, batch in tqdm(enumerate(tr_dl)):\n",
    "\n",
    "        ims, gts = to_device(batch, device)\n",
    "\n",
    "        loss, epoch_loss, epoch_acc, epoch_f1 = get_metrics(m, ims, gts, loss_fn, epoch_loss, epoch_acc, epoch_f1)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "    tr_loss_to_track = epoch_loss / len(tr_dl)\n",
    "    tr_acc_to_track  = epoch_acc  / len(tr_dl.dataset)\n",
    "    tr_f1_to_track   = epoch_f1   / len(tr_dl)\n",
    "    tr_losses.append(tr_loss_to_track); tr_accs.append(tr_acc_to_track); tr_f1s.append(tr_f1_to_track)\n",
    "\n",
    "    print(f\"{epoch + 1}-epoch train process is completed!\")\n",
    "    print(f\"{epoch + 1}-epoch train loss          -> {tr_loss_to_track:.3f}\")\n",
    "    print(f\"{epoch + 1}-epoch train accuracy      -> {tr_acc_to_track:.3f}\")\n",
    "    print(f\"{epoch + 1}-epoch train f1-score      -> {tr_f1_to_track:.3f}\")\n",
    "\n",
    "    m.eval()\n",
    "    with torch.no_grad():\n",
    "        val_epoch_loss, val_epoch_acc, val_epoch_f1 = 0, 0, 0\n",
    "        for idx, batch in enumerate(val_dl):\n",
    "            ims, gts = to_device(batch, device)\n",
    "            loss, val_epoch_loss, val_epoch_acc, val_epoch_f1 = get_metrics(m, ims, gts, loss_fn, val_epoch_loss, val_epoch_acc, val_epoch_f1)\n",
    "\n",
    "        val_loss_to_track = val_epoch_loss / len(val_dl)\n",
    "        val_acc_to_track  = val_epoch_acc  / len(val_dl.dataset)\n",
    "        val_f1_to_track   = val_epoch_f1   / len(val_dl)\n",
    "        val_losses.append(val_loss_to_track); val_accs.append(val_acc_to_track); val_f1s.append(val_f1_to_track)\n",
    "\n",
    "        print(f\"{epoch + 1}-epoch validation process is completed!\")\n",
    "        print(f\"{epoch + 1}-epoch validation loss     -> {val_loss_to_track:.3f}\")\n",
    "        print(f\"{epoch + 1}-epoch validation accuracy -> {val_acc_to_track:.3f}\")\n",
    "        print(f\"{epoch + 1}-epoch validation f1-score -> {val_f1_to_track:.3f}\")\n",
    "\n",
    "        if val_loss_to_track < (best_loss + threshold):\n",
    "            os.makedirs(save_dir, exist_ok = True)\n",
    "            best_loss = val_loss_to_track\n",
    "            torch.save(m.state_dict(), f\"{save_dir}/{save_prefix}_best_model.pth\")\n",
    "            \n",
    "        else:\n",
    "            not_improved += 1\n",
    "            print(f\"Loss value did not decrease for {not_improved} epochs\")\n",
    "            if not_improved == patience:\n",
    "                print(f\"Stop training since loss value did not decrease for {patience} epochs.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotLearningCurves:\n",
    "    \n",
    "    def __init__(self, tr_losses, val_losses, tr_accs, val_accs, tr_f1s, val_f1s):\n",
    "        \n",
    "        self.tr_losses, self.val_losses, self.tr_accs, self.val_accs, self.tr_f1s, self.val_f1s = tr_losses, val_losses, tr_accs, val_accs, tr_f1s, val_f1s\n",
    "        \n",
    "    def plot(self, array_1, array_2, label_1, label_2, color_1, color_2):\n",
    "        \n",
    "        plt.plot(array_1, label = label_1, c = color_1); plt.plot(array_2, label = label_2, c = color_2)\n",
    "        \n",
    "    def create_figure(self): plt.figure(figsize = (10, 5))\n",
    "    \n",
    "    def decorate(self, ylabel, xlabel = \"Epochs\"): \n",
    "        \n",
    "        plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "        plt.xticks(ticks = np.arange(len(self.tr_accs)), labels = [i for i in range(1, len(self.tr_accs) + 1)])\n",
    "        plt.legend(); plt.show()      \n",
    "        \n",
    "    def visualize(self):\n",
    "        \n",
    "        # Figure 1\n",
    "        self.create_figure()\n",
    "        self.plot(array_1 = self.tr_losses, array_2 = self.val_losses, label_1 = \"Train Loss\", label_2 = \"Validation Loss\", color_1 = \"tab:blue\", color_2 = \"tab:red\"); self.decorate(ylabel = \"Loss Values\")\n",
    "        \n",
    "        # Figure 2\n",
    "        self.create_figure()\n",
    "        self.plot(array_1 = self.tr_accs, array_2 = self.val_accs, label_1 = \"Train Accuracy\", label_2 = \"Validation Accuracy\", color_1 = \"tab:blue\", color_2 = \"tab:red\")\n",
    "        self.decorate(ylabel = \"Accuracy Scores\")\n",
    "        \n",
    "        # Figure 3\n",
    "        self.create_figure()\n",
    "        self.plot(array_1 = [tr_f1.cpu() for tr_f1 in self.tr_f1s], array_2 = [vl_f1.cpu() for vl_f1 in self.val_f1s], label_1 = \"Train F1 Score\", label_2 = \"Validation F1 Score\", color_1 = \"tab:blue\", color_2 = \"tab:red\"); self.decorate(ylabel = \"F1 Scores\")\n",
    "        \n",
    "PlotLearningCurves(tr_losses, val_losses, tr_accs, val_accs, tr_f1s, val_f1s).visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "class SaveFeatures():\n",
    "    \n",
    "    \"\"\" Extract pretrained activations\"\"\"\n",
    "    features = None\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features = ((output.cpu()).data).numpy()\n",
    "    def remove(self): self.hook.remove()\n",
    "\n",
    "def getCAM(conv_fs, linear_weights, class_idx):\n",
    "    \n",
    "    bs, chs, h, w = conv_fs.shape\n",
    "    cam = linear_weights[class_idx].dot(conv_fs[0,:, :, ].reshape((chs, h * w)))\n",
    "    cam = cam.reshape(h, w)\n",
    "    \n",
    "    return (cam - np.min(cam)) / np.max(cam)\n",
    "\n",
    "def inference(model, device, test_dl, num_ims, row, final_conv, fc_params, cls_names = None):\n",
    "    \n",
    "    weight, acc = np.squeeze(fc_params[0].cpu().data.numpy()), 0\n",
    "    activated_features = SaveFeatures(final_conv)\n",
    "    preds, images, lbls = [], [], []\n",
    "    for idx, batch in tqdm(enumerate(test_dl)):\n",
    "        im, gt = to_device(batch, device)\n",
    "        pred_class = torch.argmax(model(im), dim = 1)\n",
    "        acc += (pred_class == gt).sum().item()\n",
    "        images.append(im)\n",
    "        preds.append(pred_class.item())\n",
    "        lbls.append(gt.item())\n",
    "    \n",
    "    print(f\"Accuracy of the model on the test data -> {(acc / len(test_dl.dataset)):.3f}\")\n",
    "    \n",
    "    plt.figure(figsize = (20, 10))\n",
    "    indekslar = [random.randint(0, len(images) - 1) for _ in range(num_ims)]\n",
    "    \n",
    "    for idx, indeks in enumerate(indekslar):\n",
    "        \n",
    "        im = images[indeks].squeeze()\n",
    "        pred_idx = preds[indeks]\n",
    "        heatmap = getCAM(activated_features.features, weight, pred_idx)\n",
    "        \n",
    "        # Start plot\n",
    "        plt.subplot(row, num_ims // row, idx + 1)\n",
    "        plt.imshow(tensor_2_im(im), cmap = \"gray\"); plt.axis(\"off\")\n",
    "        plt.imshow(cv2.resize(heatmap, (im_size, im_size), interpolation=cv2.INTER_LINEAR), alpha=0.4, cmap='jet'); plt.axis(\"off\")\n",
    "        \n",
    "        if cls_names is not None: plt.title(f\"GT -> {cls_names[int(lbls[indeks])]} ; PRED -> {cls_names[int(preds[indeks])]}\", color=(\"green\" if {cls_names[int(lbls[indeks])]} == {cls_names[int(preds[indeks])]} else \"red\"))\n",
    "        else: plt.title(f\"GT -> {gt} ; PRED -> {pred}\")\n",
    "\n",
    "m.load_state_dict(torch.load(f\"{save_dir}/{save_prefix}_best_model.pth\"))\n",
    "m.eval()\n",
    "final_conv, fc_params = m.features[-1], list(m.head.fc.parameters())\n",
    "inference(model = m.to(device), device = device, test_dl = ts_dl, num_ims = 20, row = 4, cls_names = list(classes.keys()), final_conv = final_conv, fc_params = fc_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
